import os
import ast
import inspect
import importlib.util
import sys
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
import re
import json
from collections import defaultdict, Counter
import numpy as np

class InterconnectDiagnostic:
    """
    –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ Interconnect
    """
    
    def __init__(self):
        # –ü—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        self.framework_paths = [
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/fur/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/fur/c/"
        ]
        
        self.new_files_paths = [
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/IterFree_neural_solver/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/IterFree_spectral_core/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/MLMVN/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/Rqaoa_agents/",
            "/content/drive/MyDrive/QOKit_enhanced_MLMVN/Rqaoa_core/"
        ]
        
        self.interconnect_path = "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/"
        
        # –ö—ç—à–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.framework_files = {}
        self.new_files = {}
        self.integration_map = {}
        self.exclusion_list = set()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        self.compatibility_patterns = {
            'quantum_simulation': [
                'quantum', 'qubit', 'gate', 'circuit', 'hamiltonian', 'pauli',
                'unitary', 'density', 'state', 'measurement', 'operator',
                'eigenvalue', 'eigenvector', 'fidelity', 'entanglement'
            ],
            'neural_network': [
                'neural', 'network', 'train', 'predict', 'layer', 'neuron',
                'activation', 'loss', 'optimizer', 'gradient', 'backprop',
                'forward', 'batch', 'epoch', 'model', 'weights', 'bias'
            ],
            'optimization': [
                'optimize', 'minimize', 'maximize', 'cost', 'objective',
                'constraint', 'solver', 'algorithm', 'iteration', 'convergence'
            ],
            'data_processing': [
                'data', 'process', 'transform', 'preprocess', 'normalize',
                'feature', 'dataset', 'batch', 'pipeline', 'filter'
            ],
            'visualization': [
                'plot', 'graph', 'visualize', 'chart', 'display', 'render',
                'matplotlib', 'plotly', 'seaborn', 'figure', 'axis'
            ],
            'benchmarking': [
                'benchmark', 'performance', 'timing', 'profiling', 'metrics',
                'evaluate', 'test', 'score', 'comparison', 'analysis'
            ]
        }

    def scan_files(self, directory: str) -> Dict[str, Dict]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        files_data = {}
        
        if not os.path.exists(directory):
            return files_data
            
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith('.py') and not file.startswith('__'):
                    filepath = os.path.join(root, file)
                    try:
                        metadata = self._extract_file_metadata(filepath)
                        if metadata:
                            files_data[file] = metadata
                    except Exception as e:
                        files_data[file] = {'error': str(e), 'path': filepath}
        
        return files_data

    def _extract_file_metadata(self, filepath: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ Python —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            metadata = {
                'path': filepath,
                'file_name': os.path.basename(filepath),
                'classes': [],
                'functions': [],
                'imports': [],
                'keywords': set(),
                'docstring': '',
                'has_init': False,
                'has_main': False,
                'complexity_score': 0,
                'interaction_points': [],
                'dependencies': set(),
                'parameters': [],
                'return_types': []
            }
            
            # –û—Å–Ω–æ–≤–Ω–æ–π docstring
            metadata['docstring'] = ast.get_docstring(tree) or ''
            
            # –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        metadata['imports'].append(alias.name)
                        metadata['dependencies'].add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        metadata['imports'].append(node.module)
                        metadata['dependencies'].add(node.module)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = {
                        'name': node.name,
                        'methods': [],
                        'attributes': [],
                        'docstring': ast.get_docstring(node) or '',
                        'bases': [self._get_node_name(base) for base in node.bases]
                    }
                    
                    # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Å–∞
                    for item in node.body:
                        if isinstance(item, ast.FunctionDef):
                            method_info = self._analyze_function(item)
                            class_info['methods'].append(method_info)
                            if item.name == '__init__':
                                metadata['has_init'] = True
                    
                    metadata['classes'].append(class_info)
            
            # –ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–π
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Å–∞
                    is_method = False
                    for class_node in ast.walk(tree):
                        if isinstance(class_node, ast.ClassDef):
                            if node in class_node.body:
                                is_method = True
                                break
                    
                    if not is_method:
                        func_info = self._analyze_function(node)
                        metadata['functions'].append(func_info)
                        if node.name == 'main':
                            metadata['has_main'] = True
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            content_lower = content.lower()
            for category, keywords in self.compatibility_patterns.items():
                found_keywords = [kw for kw in keywords if kw in content_lower]
                if found_keywords:
                    metadata['keywords'].add(category)
            
            # –ü–æ–∏—Å–∫ —Ç–æ—á–µ–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            metadata['interaction_points'] = self._find_interaction_points(content)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            metadata['complexity_score'] = self._calculate_complexity(tree)
            
            return metadata
            
        except Exception as e:
            return {'error': str(e), 'path': filepath}

    def _analyze_function(self, node: ast.FunctionDef) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏"""
        func_info = {
            'name': node.name,
            'args': [arg.arg for arg in node.args.args],
            'docstring': ast.get_docstring(node) or '',
            'returns': None,
            'complexity': 0
        }
        
        # –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ —Ç–∏–ø–∞
        if node.returns:
            func_info['returns'] = ast.unparse(node.returns) if hasattr(ast, 'unparse') else str(node.returns)
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏
        func_info['complexity'] = len([n for n in ast.walk(node) if isinstance(n, (ast.If, ast.For, ast.While, ast.Try))])
        
        return func_info

    def _calculate_complexity(self, tree: ast.AST) -> int:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞"""
        complexity = 0
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(node, ast.FunctionDef):
                complexity += 1
            elif isinstance(node, ast.ClassDef):
                complexity += 2
        return complexity

    def _find_interaction_points(self, content: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–µ–∫ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–æ–¥–µ"""
        interaction_points = []
        
        patterns = [
            r'\.predict\(',
            r'\.train\(',
            r'\.optimize\(',
            r'\.simulate\(',
            r'\.execute\(',
            r'\.process\(',
            r'\.transform\(',
            r'\.fit\(',
            r'\.forward\(',
            r'\.backward\(',
            r'route_request\(',
            r'interconnect\.'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            if matches:
                interaction_points.extend(matches)
        
        return interaction_points

    def _get_node_name(self, node) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —É–∑–ª–∞ AST"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_node_name(node.value)}.{node.attr}"
        return str(node)

    def _functions_similarity(self, func1: str, func2: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ—É–Ω–∫—Ü–∏–π"""
        words1 = set(re.findall(r'[a-zA-Z]+', func1.lower()))
        words2 = set(re.findall(r'[a-zA-Z]+', func2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0

    def analyze_compatibility(self, framework_file: Dict, new_file: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∏ –Ω–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏"""
        if 'error' in framework_file or 'error' in new_file:
            return {'compatible': False, 'reason': 'parsing_error', 'score': 0.0}
        
        score = 0.0
        reasons = []
        
        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        common_keywords = framework_file['keywords'] & new_file['keywords']
        if common_keywords:
            keyword_score = len(common_keywords) / len(framework_file['keywords'] | new_file['keywords'])
            score += keyword_score * 0.4
            reasons.append(f"common_keywords: {list(common_keywords)}")
        
        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
        fw_deps = framework_file.get('dependencies', set())
        new_deps = new_file.get('dependencies', set())
        
        if fw_deps and new_deps:
            deps_intersection = len(fw_deps & new_deps)
            deps_union = len(fw_deps | new_deps)
            if deps_union > 0:
                deps_score = deps_intersection / deps_union
                score += deps_score * 0.3
                reasons.append(f"common_dependencies: {deps_intersection}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–π
        fw_functions = [f['name'] for f in framework_file.get('functions', [])]
        new_functions = [f['name'] for f in new_file.get('functions', [])]
        
        similar_functions = 0
        for nf in new_functions:
            for ff in fw_functions:
                if self._functions_similarity(nf, ff) > 0.7:
                    similar_functions += 1
                    break
        
        if new_functions:
            func_score = similar_functions / len(new_functions)
            score += func_score * 0.2
            reasons.append(f"similar_functions: {similar_functions}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤
        if framework_file.get('classes') and new_file.get('classes'):
            score += 0.15
            reasons.append("both_have_classes")
        
        # –¢–æ—á–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        fw_interactions = set(framework_file.get('interaction_points', []))
        new_interactions = set(new_file.get('interaction_points', []))
        
        if fw_interactions & new_interactions:
            score += 0.1
            reasons.append("interaction_points_match")
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à—É—é —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity_diff = abs(framework_file['complexity_score'] - new_file['complexity_score'])
        if complexity_diff > 50:
            score -= 0.1
            reasons.append("complexity_mismatch")
        
        return {
            'compatible': score > 0.25,
            'score': score,
            'reasons': reasons,
            'common_keywords': list(common_keywords) if common_keywords else [],
            'common_dependencies': list(fw_deps & new_deps) if fw_deps and new_deps else []
        }

    def generate_exclusion_list(self) -> Set[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        exclusions = set()
        
        service_patterns = [
            '__init__', '__pycache__', 'test_', '_test', 'benchmark', 'example', 
            'demo', 'tutorial', 'setup', 'config', 'settings', 'constants'
        ]
        
        for framework_file, metadata in self.framework_files.items():
            if 'error' in metadata:
                exclusions.add(framework_file)
                continue
                
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            for pattern in service_patterns:
                if pattern in framework_file.lower():
                    exclusions.add(framework_file)
                    break
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã —Ç–æ–ª—å–∫–æ —Å –∏–º–ø–æ—Ä—Ç–∞–º–∏
            if (not metadata.get('classes') and 
                not metadata.get('functions') and 
                len(metadata.get('imports', [])) < 3):
                exclusions.add(framework_file)
        
        return exclusions

    def generate_integration_recommendations(self) -> Dict[str, List[Tuple[str, float]]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        recommendations = {}
        
        for new_file, new_metadata in self.new_files.items():
            if 'error' in new_metadata:
                continue
                
            best_matches = []
            
            for framework_file, framework_metadata in self.framework_files.items():
                if framework_file in self.exclusion_list:
                    continue
                    
                compatibility = self.analyze_compatibility(framework_metadata, new_metadata)
                
                if compatibility['compatible']:
                    best_matches.append((framework_file, compatibility['score']))
            
            best_matches.sort(key=lambda x: x[1], reverse=True)
            recommendations[new_file] = best_matches[:5]
        
        return recommendations

    def run_diagnostic(self) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        print("–ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
        print("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞...")
        for path in self.framework_paths:
            files = self.scan_files(path)
            self.framework_files.update(files)
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        print("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        for path in self.new_files_paths:
            files = self.scan_files(path)
            self.new_files.update(files)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        self.exclusion_list = self.generate_exclusion_list()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print("–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ñ–∞–π–ª–æ–≤...")
        recommendations = self.generate_integration_recommendations()
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report = {
            'summary': {
                'framework_files_total': len(self.framework_files),
                'framework_files_valid': len([f for f in self.framework_files.values() if 'error' not in f]),
                'new_files_total': len(self.new_files),
                'new_files_valid': len([f for f in self.new_files.values() if 'error' not in f]),
                'excluded_files': len(self.exclusion_list),
                'integration_candidates': len(recommendations)
            },
            'integration_recommendations': recommendations,
            'excluded_files': list(self.exclusion_list),
            'framework_analysis': {
                name: {
                    'classes': len(meta.get('classes', [])),
                    'functions': len(meta.get('functions', [])),
                    'keywords': list(meta.get('keywords', set())),
                    'complexity': meta.get('complexity_score', 0),
                    'interaction_points': meta.get('interaction_points', [])
                }
                for name, meta in self.framework_files.items()
                if 'error' not in meta
            },
            'new_files_analysis': {
                name: {
                    'classes': len(meta.get('classes', [])),
                    'functions': len(meta.get('functions', [])),
                    'keywords': list(meta.get('keywords', set())),
                    'complexity': meta.get('complexity_score', 0),
                    'interaction_points': meta.get('interaction_points', [])
                }
                for name, meta in self.new_files.items()
                if 'error' not in meta
            }
        }
        
        return report

    def generate_integration_code(self, new_file: str, framework_files: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Interconnect"""
        code_template = f"""
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è {new_file} —á–µ—Ä–µ–∑ Interconnect
from interconnect import route_request, register_component, on_event

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
# register_component('{new_file.replace('.py', '')}', YourComponentClass)

# –ü—Ä–∏–º–µ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ñ–∞–π–ª–∞–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞:
"""
        for fw_file in framework_files:
            code_template += f"""
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å {fw_file}
# result = route_request('{fw_file.replace('.py', '')}', 'method_name', {{
#     'method_params': {{'param': 'value'}}
# }})
"""
        
        return code_template

    def save_report(self, report: Dict, filename: str = "integration_report.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        output_path = os.path.join(self.interconnect_path, filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

    def export_integration_config(self, output_path: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ JSON"""
        recommendations = self.generate_integration_recommendations()
        
        config = {
            'framework_analysis': {
                file_path: {
                    'file_name': analysis['file_name'],
                    'classes': [c['name'] for c in analysis.get('classes', [])],
                    'functions': [f['name'] for f in analysis.get('functions', [])],
                    'keywords': list(analysis.get('keywords', set())),
                    'complexity_score': analysis.get('complexity_score', 0)
                }
                for file_path, analysis in self.framework_files.items()
                if 'error' not in analysis
            },
            'new_files_analysis': {
                file_path: {
                    'file_name': analysis['file_name'],
                    'classes': [c['name'] for c in analysis.get('classes', [])],
                    'functions': [f['name'] for f in analysis.get('functions', [])],
                    'keywords': list(analysis.get('keywords', set())),
                    'complexity_score': analysis.get('complexity_score', 0)
                }
                for file_path, analysis in self.new_files.items()
                if 'error' not in analysis
            },
            'integration_recommendations': {
                new_file: [
                    {'framework_file': match[0], 'score': match[1]} 
                    for match in matches
                ]
                for new_file, matches in recommendations.items()
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")

    def print_integration_summary(self, report: Dict):
        """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        print("\n" + "="*80)
        print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –ß–ï–†–ï–ó INTERCONNECT")
        print("="*80)
        
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"‚Ä¢ –§–∞–π–ª–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞: {report['summary']['framework_files_valid']}")
        print(f"‚Ä¢ –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {report['summary']['new_files_valid']}")
        print(f"‚Ä¢ –ò—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {report['summary']['excluded_files']}")
        print(f"‚Ä¢ –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {report['summary']['integration_candidates']}")
        
        print(f"\n–ò–°–ö–õ–Æ–ß–ï–ù–ù–´–ï –§–ê–ô–õ–´ –§–†–ï–ô–ú–í–û–†–ö–ê:")
        for excluded in sorted(report['excluded_files']):
            print(f"‚Ä¢ {excluded}")
        
        print(f"\n–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        for new_file, matches in report['integration_recommendations'].items():
            if matches:
                print(f"\n* {new_file} -> —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å:")
                for match in matches[:3]:
                    print(f"  - {match[0]} (score: {match[1]:.3f})")
            else:
                print(f"\n* {new_file} -> –ù–ï –ù–ê–ô–î–ï–ù–´ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ–∞–π–ª—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞")
        
        print(f"\n–ü–†–ò–ú–ï–†–´ –ö–û–î–ê –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        for new_file, matches in list(report['integration_recommendations'].items())[:3]:
            if not matches:
                continue
                
            new_file_name = new_file.replace('.py', '')
            best_match = matches[0]
            framework_file_name = best_match[0].replace('.py', '')
            
            print(f"\n# –ü—Ä–∏–º–µ—Ä –¥–ª—è {new_file_name}:")
            print(f"from interconnect import route_request")
            print(f"result = route_request('{framework_file_name}', 'method_name', {{")
            print(f"    'method_params': {{'data': your_data}}")
            print(f"}})")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    diagnostic = InterconnectDiagnostic()
    
    try:
        report = diagnostic.run_diagnostic()
        diagnostic.save_report(report)
        diagnostic.export_integration_config(
            os.path.join(diagnostic.interconnect_path, 'integration_config.json')
        )
        diagnostic.print_integration_summary(report)
        
        print(f"\n‚úÖ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {diagnostic.interconnect_path}integration_report.json")
        print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {diagnostic.interconnect_path}integration_config.json")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()